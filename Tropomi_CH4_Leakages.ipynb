{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is investigating the known methane leakages listed on the 'gas_leakage_incident.xlsx'.\n",
    "The library Basemap is used to create the plots.\n",
    "The plots are condenced in videos with data from surrounding days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "netCDF4 version:  1.5.3\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "# https://matplotlib.org/basemap/users/installing.html\n",
    "from matplotlib.patches import Circle\n",
    "# pip install netCDF4\n",
    "import netCDF4 as nc4\n",
    "from netCDF4 import Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import date as dt\n",
    "import numpy as np\n",
    "import cv2\n",
    "# pip install Shapely\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "\n",
    "print(\"netCDF4 version: \",nc4.__version__)\n",
    "\n",
    "##### Settings #####\n",
    "# shared folder\n",
    "basepath = \"data\\data\"\n",
    "# number of days surrounding the leakage\n",
    "n_days = 10\n",
    "# area to plot\n",
    "zoom_area = 8 # higher bigger area\n",
    "# flag to keep the temp files\n",
    "keep_temp = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of all files on the folder\n",
    "files = []\n",
    "for entry in os.listdir(basepath):\n",
    "    file=os.path.join(basepath, entry)\n",
    "    files.append(file)\n",
    "\n",
    "# Get the dates of the files\n",
    "files_date = []\n",
    "for file in files:\n",
    "    netCDF4_file = Dataset(file, mode='r')\n",
    "    files_date.append(datetime.fromtimestamp(netCDF4_file.time_reference_seconds_since_1970).date())\n",
    "\n",
    "# Read Zulka's excel file\n",
    "leakages_dataframe = pd.read_excel('gas_leakage_incident.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used functions\n",
    "def delta_longitude(latitude, distance):\n",
    "    # function to calculate the diference in longitude given a latitude and a distance\n",
    "    # approximate radius of earth in km\n",
    "    R = 6373.0\n",
    "    lat = np.deg2rad(latitude)\n",
    "    # derivated from Haversine formula\n",
    "    return np.rad2deg(2 * np.arcsin(np.sqrt( (np.sin(distance/(2*R)) ** 2) / (np.cos(lat) ** 2) )))\n",
    "\n",
    "def find_nearest(array1, array2, value1, value2):\n",
    "    # find the closest value in array, return the 2D index\n",
    "    array1 = np.asarray(array1)\n",
    "    array2 = np.asarray(array2)\n",
    "    return np.unravel_index(np.argmin(((array1 - value1)**2 + (array2 - value2)**2)), array1.shape)\n",
    "\n",
    "def distance(s,e):\n",
    "    # Function to calculate the distance between two coordinates\n",
    "    # approximate radius of earth in km\n",
    "    s_lat, s_lng = s\n",
    "    e_lat, e_lng = e\n",
    "    R = 6373.0\n",
    "    \n",
    "    s_lat = s_lat*np.pi/180.0\n",
    "    s_lng = np.deg2rad(s_lng)\n",
    "    e_lat = np.deg2rad(e_lat)\n",
    "    e_lng = np.deg2rad(e_lng)\n",
    "    \n",
    "    # Haversine formula\n",
    "    d = np.sin((e_lat - s_lat)/2)**2 + np.cos(s_lat)*np.cos(e_lat) * np.sin((e_lng - s_lng)/2)**2\n",
    "    \n",
    "    return (2 * R * np.arcsin(np.sqrt(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### Scan ######\n",
    "# check if all necessary data is present and config the list\n",
    "# create a list of lists with all files from the leakages\n",
    "# The high level is the leakage event\n",
    "# the second level are the dates\n",
    "# the third level are the files\n",
    "# [[event1,[dates,[files]]]]\n",
    "leakage_files = []\n",
    "\n",
    "# scan all incidents listed on the excel file\n",
    "# so far just the Assam leakage, for all remove '.head(1)'\n",
    "for index, row in leakages_dataframe.iterrows():\n",
    "    if pd.isna(row['latitude']): continue # avoid rows without data\n",
    "    \n",
    "    # get the dates of the surrounding days\n",
    "    date_list = [row['date'].to_pydatetime().date() - timedelta(days=x) for x in range(n_days+1)]\n",
    "    plus_date_list = [row['date'].to_pydatetime().date() + timedelta(days=x) for x in range(n_days+1)]\n",
    "    date_list.reverse()\n",
    "    date_list = date_list + plus_date_list[1:]\n",
    "    del plus_date_list\n",
    "\n",
    "    # check each date if there is a file of the region\n",
    "    lat = row['latitude']\n",
    "    long = row['longitude']\n",
    "    leakage_files_second = []\n",
    "    # for all dates in this location\n",
    "    for date in date_list:\n",
    "        sub_list = []\n",
    "        file_exist = False\n",
    "\n",
    "        # scan all files\n",
    "        for i in range(len(files)):\n",
    "            #### Temporal alignment ####\n",
    "            # check if the date is the same\n",
    "            if files_date[i] != date:\n",
    "                continue\n",
    "            \n",
    "            # open the file safely\n",
    "            file = files[i]\n",
    "            try:\n",
    "                netCDF4_file = Dataset(file, mode='r')\n",
    "            except:\n",
    "                print('Error on file: ', file)\n",
    "                continue\n",
    "            \n",
    "            #### Spatial alignment ####\n",
    "            # get the scanline with the aprox. latitude\n",
    "            for scanline in range(int(netCDF4_file.groups['PRODUCT'].variables['scanline'][-1])):\n",
    "                # scanline from 0 to total (e.g 3736) means latitude from -90 to 90\n",
    "                # this means the data is stored from South to North (upside dowm)\n",
    "                # 107 is aproximate middle of the scanline\n",
    "                if netCDF4_file.groups['PRODUCT'].variables['latitude'][:][0,scanline,107] > lat: break\n",
    "            # now check if the longitude is in this orbit\n",
    "            minimum = netCDF4_file.groups['PRODUCT'].variables['longitude'][:][0,scanline,:].min()\n",
    "            maximum = netCDF4_file.groups['PRODUCT'].variables['longitude'][:][0,scanline,:].max()\n",
    "            # consider a 20 degres window to include the file\n",
    "            if minimum - 10 < long and maximum + 10 > long:\n",
    "                sub_list.append(file)\n",
    "                file_exist = True\n",
    "            netCDF4_file.close()\n",
    "        # if there is no file in the folder add a note of missing file\n",
    "        if not file_exist: sub_list.append('missing ' + str(row['show_text']) +' ('+ str(date)+ ')')\n",
    "        \n",
    "        # add the sub list to second list\n",
    "        leakage_files_second.append([date,sub_list])\n",
    "        del sub_list\n",
    "    leakage_files.append([row['show_text'],lat,long, leakage_files_second])\n",
    "\n",
    "# leakage_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: MatplotlibDeprecationWarning: \n",
      "The dedent function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use inspect.cleandoc instead.\n",
      "C:\\Users\\natan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:63: MatplotlibDeprecationWarning: \n",
      "The dedent function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use inspect.cleandoc instead.\n"
     ]
    }
   ],
   "source": [
    "##### Create the images and the video #####\n",
    "\n",
    "# scan the events listed to create a series of plots and make a video of it\n",
    "for event in leakage_files: # first level of the list: events\n",
    "\n",
    "    animation_list = []\n",
    "    show_text,center_lat,center_long, date_list = event\n",
    "    # create a temp folder\n",
    "    if not os.path.exists('temp/' + show_text):\n",
    "        os.makedirs('temp/' + show_text)\n",
    "    \n",
    "    \n",
    "    for date, files2 in date_list: # second level of the list: dates\n",
    "        # setup the plot\n",
    "        fig = plt.figure(figsize=(16,9))\n",
    "        ax = plt.gca()\n",
    "        # for high res: conda install basemap-data-hires\n",
    "        m = Basemap(projection='cyl',\n",
    "                    urcrnrlat = center_lat + zoom_area, # top latitude\n",
    "                    llcrnrlat = center_lat - zoom_area, # low latitude\n",
    "                    urcrnrlon = center_long + 2*zoom_area, # right longitude\n",
    "                    llcrnrlon = center_long - 2*zoom_area, # left longitude\n",
    "                    resolution='i') # resolution='c' low res 'h' high res 'i' medium\n",
    "        \n",
    "        # Add grid lines\n",
    "        m.drawparallels(np.arange(-80., 81., 10.), labels=[1,0,0,0], fontsize=10)\n",
    "        m.drawmeridians(np.arange(-180., 181., 10.), labels=[0,0,0,1], fontsize=10)\n",
    "        # Draw the NASA ‘Blue Marble’ image.\n",
    "        m.bluemarble()\n",
    "        \n",
    "        # draw a circle around the event with radius of 50 km\n",
    "        ax.add_patch(plt.Circle(m(center_long,center_lat), delta_longitude(center_lat,50), color='red',fill=False))\n",
    "        \n",
    "        for file in files2: # third level of the list: files\n",
    "            # avoid missing files\n",
    "            if 'missing' in file: continue\n",
    "            netCDF4_file = Dataset(file, mode='r')\n",
    "            # get the scanline with the aproximated latitude\n",
    "            for scanline in range(int(netCDF4_file.groups['PRODUCT'].variables['scanline'][-1])):\n",
    "                # scanline from 0 to total (e.g 3736) means latitude from -90 to 90\n",
    "                # this means the data is stored from South to North (upside dowm)\n",
    "                # 107 is aproximate middle of the scanline\n",
    "                if netCDF4_file.groups['PRODUCT'].variables['latitude'][:][0,scanline,107] > center_lat: break\n",
    "            \n",
    "            # create a general plane to see the covered area\n",
    "            l_min = scanline - 25 * zoom_area # \n",
    "            l_max = scanline + 25 * zoom_area # 400x215\n",
    "            plane_data = np.array([[1800 for i in range(215)] for j in range(l_max-l_min)])\n",
    "\n",
    "            # Convert the 3d array to a 2d array\n",
    "            lons = netCDF4_file.groups['PRODUCT'].variables['longitude'][:][0,l_min:l_max,:]\n",
    "            lats = netCDF4_file.groups['PRODUCT'].variables['latitude'][:][0,l_min:l_max,:]\n",
    "            ch4 = netCDF4_file.groups['PRODUCT'].variables['methane_mixing_ratio'][0,l_min:l_max,:]\n",
    "\n",
    "            # Draw reach of each file in format of a plane\n",
    "            m.pcolor(lons, lats, plane_data, cmap='jet', alpha=0.05)\n",
    "\n",
    "            # Draw ch4 data, set vmin and vmax to keep the same on all plots\n",
    "            cs = m.pcolor(lons, lats, ch4, cmap='jet', vmin=1500, vmax=2000)\n",
    "            netCDF4_file.close()\n",
    "        # draw contours\n",
    "        m.drawcoastlines()\n",
    "        m.drawcountries()\n",
    "        # show the colorbar\n",
    "        cbar = m.colorbar(cs, location='bottom', pad=\"10%\")\n",
    "        plt.title(show_text + ' (' + str(date) + ')')\n",
    "        plt.tight_layout()\n",
    "        # save the image in a temp folder\n",
    "        plt.savefig('temp/' + show_text + '/' + str(date)+'.png')\n",
    "        plt.close(fig) # dont show the image\n",
    "    \n",
    "    # read all files from the temp folder and create a video\n",
    "    img_array = []\n",
    "    for file in os.listdir('temp/' + show_text + '/'):\n",
    "        img = cv2.imread('temp/' + show_text + '/' + file)\n",
    "        img_array.append(img)\n",
    "        height, width, layers = img.shape\n",
    "        size = (width,height)\n",
    "        # delete temp files\n",
    "        if not keep_temp: os.remove( 'temp/' + show_text + '/' + file)\n",
    "\n",
    "    # create a video with 0.5 FPS\n",
    "    out = cv2.VideoWriter(show_text + '.avi',cv2.VideoWriter_fourcc(*'DIVX'), 0.5, size)\n",
    "    for img in img_array:\n",
    "        out.write(img)\n",
    "    out.release()\n",
    "    del img_array\n",
    "    \n",
    "    # delete the temp folder\n",
    "    if not keep_temp: os.rmdir('temp/' + show_text + '/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Data Fusion ######\n",
    "# This cell fuses the data from two orbits in the same day\n",
    "# It compares the location of the data and average if both are valid\n",
    "\n",
    "# Same structure from leakage_files but now for data\n",
    "# The high level is the leakage event\n",
    "# the second level are the dates\n",
    "# the third level is data\n",
    "# [[event1,center_lat, center_lon,[dates,[data]]]]\n",
    "leakage_data = []\n",
    "\n",
    "for event in leakage_files: # first level of the list: events\n",
    "    show_text,center_lat,center_long, date_list = event\n",
    "    leak_data = []\n",
    "    for date, files2 in date_list: # second level of the list: dates\n",
    "        \n",
    "        if len(files2) >= 2:\n",
    "            # take up to 2 files\n",
    "            if len(files2) == 2: file1, file2 = files2\n",
    "            if len(files2) > 2:\n",
    "                file1 = files2[0]\n",
    "                file2 = files2[1]\n",
    "            # check if the file is present\n",
    "            if 'missing' in file1: continue\n",
    "            if 'missing' in file2: continue\n",
    "            # open the files safely\n",
    "            try:\n",
    "                netCDF4_file1 = Dataset(file1, mode='r')\n",
    "                netCDF4_file2 = Dataset(file2, mode='r')\n",
    "            except:\n",
    "                print('Error on files: ')\n",
    "                print(files1)\n",
    "                print(files2)\n",
    "            # get the scanline with the aproximated latitude\n",
    "            for scanline1 in range(int(netCDF4_file1.groups['PRODUCT'].variables['scanline'][-1])):\n",
    "                # scanline from 0 to total (e.g 3736) means latitude from -90 to 90\n",
    "                # this means the data is stored from South to North (upside dowm)\n",
    "                # 107 is aproximate middle of the scanline\n",
    "                if netCDF4_file1.groups['PRODUCT'].variables['latitude'][:][0,scanline1,107] > center_lat: break\n",
    "            # ~4000x215\n",
    "            l_min = scanline1 - 25 * zoom_area\n",
    "            l_max = scanline1 + 25 * zoom_area\n",
    "            # 400x215\n",
    "            \n",
    "            # Convert the 3d array to a 2d array\n",
    "            lons1 = netCDF4_file1.groups['PRODUCT'].variables['longitude'][:][0,l_min:l_max,:]\n",
    "            lats1 = netCDF4_file1.groups['PRODUCT'].variables['latitude'][:][0,l_min:l_max,:]\n",
    "            ch41 = netCDF4_file1.groups['PRODUCT'].variables['methane_mixing_ratio'][0,l_min:l_max,:]\n",
    "            # get the scanline with the aproximated latitude\n",
    "            for scanline2 in range(int(netCDF4_file2.groups['PRODUCT'].variables['scanline'][-1])):\n",
    "                # scanline from 0 to total (e.g 3736) means latitude from -90 to 90\n",
    "                # this means the data is stored from South to North (upside dowm)\n",
    "                # 107 is aproximate middle of the scanline\n",
    "                if netCDF4_file2.groups['PRODUCT'].variables['latitude'][:][0,scanline2,107] > center_lat: break\n",
    "            \n",
    "            # ~4000x215\n",
    "            l_min = scanline2 - 25 * zoom_area\n",
    "            l_max = scanline2 + 25 * zoom_area\n",
    "            # 400x215\n",
    "            \n",
    "            # Convert the 3d array to a 2d array\n",
    "            lons2 = netCDF4_file2.groups['PRODUCT'].variables['longitude'][:][0,l_min:l_max,:]\n",
    "            lats2 = netCDF4_file2.groups['PRODUCT'].variables['latitude'][:][0,l_min:l_max,:]\n",
    "            ch42 = netCDF4_file2.groups['PRODUCT'].variables['methane_mixing_ratio'][0,l_min:l_max,:]\n",
    "            \n",
    "            # Scan for the overlaping area\n",
    "            area1 = Polygon([(lats1[0,-1], lons1[0,-1]), (lats1[0,0], lons1[0,0]), (lats1[0,-1], lons1[-1,0]), (lats1[-1,-1], lons1[-1,-1])])\n",
    "            area2 = Polygon([(lats2[0,-1], lons2[0,-1]), (lats2[0,0], lons2[0,0]), (lats2[0,-1], lons2[-1,0]), (lats2[-1,-1], lons2[-1,-1])])\n",
    "            x = area2.intersection(area1)\n",
    "            if x.area != 0.0: # if the area is zero there is no overlaping\n",
    "                # scan for coordenates in the overlap\n",
    "                for i in range(len(lats2)):\n",
    "                    for j in range(len(lats2[i,:])):\n",
    "                        # get the coordenates\n",
    "                        lat2 = lats2[i,j]\n",
    "                        lon2 = lons2[i,j]\n",
    "                        # library shapely can check geometrically if the coordenate is conteined\n",
    "                        if x.contains(Point(lat2, lon2)):\n",
    "                            # the data is overlaping\n",
    "                            index = find_nearest(lats1,lons1,lat2,lon2)\n",
    "                            # index is related to file1\n",
    "                            dist = distance((lat2,lon2),(lats1[index],lons1[index])) # distance in km\n",
    "                            # Dry-air mixing ratio of methane for cloud-free observations with a spatial\n",
    "                            # resolution of 7x7km2 observed at about 13:30 local solar time\n",
    "                            if dist < 9.89: # diagonal of 7x7km\n",
    "                                print('found a candidate')\n",
    "                                # consider to be the same value\n",
    "                                # the value in ch41 is not valid\n",
    "                                if np.ma.getmask(ch41)[index]: break\n",
    "                                # the value in ch42 is not valid\n",
    "                                if np.ma.getmask(ch42)[i,j]: break\n",
    "                                \n",
    "                                ####### Data normalization #########\n",
    "                                # normalize the values according to the quality index\n",
    "                                # get the quality index\n",
    "                                # A continuous quality descriptor, varying between 0 (no data) and 1 (full quality data)\n",
    "                                qi1_array = netCDF4_file1.groups['PRODUCT'].variables['qa_value'][:][0,l_min:l_max,:]\n",
    "                                qi2_array = netCDF4_file2.groups['PRODUCT'].variables['qa_value'][:][0,l_min:l_max,:]\n",
    "                                qi1 = qi1_array[index] # index is related to file1\n",
    "                                qi2 = qi1_array[i,j]   # i, j are related to file2\n",
    "                                \n",
    "                                # mask the value on the ch41 and insert the average on ch2\n",
    "                                value1 = ch41[index]\n",
    "                                value2 = ch42[i,j]\n",
    "                                ch41[index] = np.ma.masked\n",
    "                                ch42[i,j] = (value1*qi1 + value2*qi2) / (qi1+qi2)\n",
    "                                \n",
    "            lats = np.ma.vstack((lats1, lats2))\n",
    "            lons = np.ma.vstack((lons1, lons2))\n",
    "            ch4 = np.ma.vstack((ch41, ch42))\n",
    "            netCDF4_file1.close()\n",
    "            netCDF4_file2.close()\n",
    "        if len(files2) == 1: # for a single file\n",
    "            # check if the file is present\n",
    "            if 'missing' in files2[0]: continue\n",
    "            # open the file\n",
    "            netCDF4_file = Dataset(files2[0], mode='r')\n",
    "            # get the scanline with the aproximated latitude\n",
    "            for scanline in range(int(netCDF4_file.groups['PRODUCT'].variables['scanline'][-1])):\n",
    "                if netCDF4_file.groups['PRODUCT'].variables['latitude'][:][0,scanline,107] > center_lat: break\n",
    "            # ~4000x215\n",
    "            l_min = scanline - 25 * zoom_area\n",
    "            l_max = scanline + 25 * zoom_area\n",
    "            # 400x215\n",
    "            lats = netCDF4_file.groups['PRODUCT'].variables['latitude'][:][0,l_min:l_max,:]\n",
    "            lons = netCDF4_file.groups['PRODUCT'].variables['longitude'][:][0,l_min:l_max,:]\n",
    "            ch4 = netCDF4_file.groups['PRODUCT'].variables['methane_mixing_ratio'][0,l_min:l_max,:]\n",
    "            netCDF4_file.close()\n",
    "        leak_data.append([date, lats, lons, ch4])\n",
    "    leakage_data.append([show_text, center_lat, center_long, leak_data])\n",
    "\n",
    "# leakage_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Create the images and the video #####\n",
    "\n",
    "# scan the events listed to create a series of plots and make a video of it\n",
    "for event in leakage_data: # first level of the list: events\n",
    "    animation_list = []\n",
    "    show_text,center_lat,center_long, date_list = event\n",
    "    \n",
    "    # create a temp folder\n",
    "    if not os.path.exists('temp2/' + show_text):\n",
    "        os.makedirs('temp2/' + show_text)\n",
    "    \n",
    "    for date, lats, lons, ch4 in date_list: # second level of the list: dates\n",
    "        # setup the plot\n",
    "        fig = plt.figure(figsize=(16,9))\n",
    "        ax = plt.gca()\n",
    "        # for high res: conda install basemap-data-hires\n",
    "        m = Basemap(projection='cyl',\n",
    "                    urcrnrlat = center_lat + zoom_area, # top latitude\n",
    "                    llcrnrlat = center_lat - zoom_area, # low latitude\n",
    "                    urcrnrlon = center_long + 2*zoom_area, # right longitude\n",
    "                    llcrnrlon = center_long - 2*zoom_area, # left longitude\n",
    "                    resolution='i') # resolution='c' low res 'h' high res 'i' medium\n",
    "        \n",
    "        # Add grid lines\n",
    "        m.drawparallels(np.arange(-80., 81., 10.), labels=[1,0,0,0], fontsize=10)\n",
    "        m.drawmeridians(np.arange(-180., 181., 10.), labels=[0,0,0,1], fontsize=10)\n",
    "        \n",
    "        # Draw the NASA ‘Blue Marble’ image.\n",
    "        m.bluemarble()\n",
    "        \n",
    "        # draw a circle around the event with radius of 50 km\n",
    "        ax.add_patch(plt.Circle(m(center_long,center_lat), delta_longitude(center_lat,50), color='red',fill=False))\n",
    "        \n",
    "        # Draw ch4 data, set vmin and vmax to keep the same on all plots\n",
    "        sc = plt.scatter(lons, lats, c=ch4, cmap='jet', vmin=1500, vmax=2000, alpha=0.7, s=8)\n",
    "        \n",
    "        # draw contours\n",
    "        m.drawcoastlines()\n",
    "        m.drawcountries()\n",
    "        # show the colorbar\n",
    "        cbar = m.colorbar(sc, location='bottom', pad=\"10%\")\n",
    "        plt.title(show_text + ' (' + str(date) + ')')\n",
    "        plt.tight_layout()\n",
    "        # save the image in a temp folder\n",
    "        plt.savefig('temp2/' + show_text + '/' + str(date)+'.png')\n",
    "        plt.close(fig) # dont show the image\n",
    "        \n",
    "    # read all files from the temp folder and create a video\n",
    "    img_array = []\n",
    "    for file in os.listdir('temp2/' + show_text + '/'):\n",
    "        img = cv2.imread('temp2/' + show_text + '/' + file)\n",
    "        img_array.append(img)\n",
    "        height, width, layers = img.shape\n",
    "        size = (width,height)\n",
    "        # delete temp files\n",
    "        if not keep_temp: os.remove('temp2/' + show_text + '/' + file)\n",
    "\n",
    "    # create a video with 0.5 FPS\n",
    "    out = cv2.VideoWriter(show_text + ' - fused.avi',cv2.VideoWriter_fourcc(*'DIVX'), 0.5, size)\n",
    "    for img in img_array:\n",
    "        out.write(img)\n",
    "    out.release()\n",
    "    del img_array\n",
    "    \n",
    "# delete the temp folder\n",
    "if not keep_temp: os.rmdir('temp2/' + show_text + '/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### END #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leakage_files = [['Assam leakage 27-05-2020',\n",
    "#   27.604277777777778,\n",
    "#   95.4046388888889,\n",
    "#   [[dt(2020, 5, 17),\n",
    "#     ['data\\\\data\\\\S5P_OFFL_L2__CH4____20200517T050007_20200517T064137_13433_01_010302_20200518T214726.nc',\n",
    "#      'data\\\\data\\\\S5P_OFFL_L2__CH4____20200517T064137_20200517T082308_13434_01_010302_20200518T233942.nc']],\n",
    "#    [dt(2020, 5, 18),\n",
    "#     ['data\\\\data\\\\S5P_OFFL_L2__CH4____20200518T044109_20200518T062239_13447_01_010302_20200519T212648.nc',\n",
    "#      'data\\\\data\\\\S5P_OFFL_L2__CH4____20200518T062239_20200518T080409_13448_01_010302_20200519T232443.nc']],\n",
    "#    [dt(2020, 5, 19),\n",
    "#     ['data\\\\data\\\\S5P_OFFL_L2__CH4____20200519T060340_20200519T074510_13462_01_010302_20200520T224831.nc']],\n",
    "#    [dt(2020, 5, 20),\n",
    "#     ['data\\\\data\\\\S5P_OFFL_L2__CH4____20200520T054442_20200520T072612_13476_01_010302_20200521T223938.nc',\n",
    "#      'data\\\\data\\\\S5P_OFFL_L2__CH4____20200520T072612_20200520T090742_13477_01_010302_20200521T235814.nc']],\n",
    "#    [dt(2020, 5, 21),\n",
    "#     ['data\\\\data\\\\S5P_OFFL_L2__CH4____20200521T052543_20200521T070713_13490_01_010302_20200522T220813.nc',\n",
    "#      'data\\\\data\\\\S5P_OFFL_L2__CH4____20200521T070713_20200521T084843_13491_01_010302_20200522T234241.nc']],\n",
    "#    [dt(2020, 5, 22),\n",
    "#     ['data\\\\data\\\\S5P_OFFL_L2__CH4____20200522T050644_20200522T064814_13504_01_010302_20200523T215000.nc',\n",
    "#      'data\\\\data\\\\S5P_OFFL_L2__CH4____20200522T064814_20200522T082944_13505_01_010302_20200523T231600.nc']],\n",
    "#    [dt(2020, 5, 23),\n",
    "#     ['data\\\\data\\\\S5P_OFFL_L2__CH4____20200523T044745_20200523T062915_13518_01_010302_20200524T212530.nc',\n",
    "#      'data\\\\data\\\\S5P_OFFL_L2__CH4____20200523T062915_20200523T081045_13519_01_010302_20200524T231032.nc']],\n",
    "#    [dt(2020, 5, 24),\n",
    "#     ['data\\\\data\\\\S5P_OFFL_L2__CH4____20200524T042846_20200524T061016_13532_01_010302_20200525T210215.nc',\n",
    "#      'data\\\\data\\\\S5P_OFFL_L2__CH4____20200524T061016_20200524T075146_13533_01_010302_20200525T225044.nc']],\n",
    "#    [dt(2020, 5, 25),\n",
    "#     ['data\\\\data\\\\S5P_OFFL_L2__CH4____20200525T055117_20200525T073247_13547_01_010302_20200526T223409.nc']],\n",
    "#    [dt(2020, 5, 26),\n",
    "#     ['data\\\\data\\\\S5P_OFFL_L2__CH4____20200526T053217_20200526T071347_13561_01_010302_20200527T222221.nc',\n",
    "#      'data\\\\data\\\\S5P_OFFL_L2__CH4____20200526T071347_20200526T085517_13562_01_010302_20200527T235240.nc']],\n",
    "#    [dt(2020, 5, 27),\n",
    "#     ['data\\\\data\\\\S5P_OFFL_L2__CH4____20200527T051318_20200527T065448_13575_01_010302_20200528T220114.nc',\n",
    "#      'data\\\\data\\\\S5P_OFFL_L2__CH4____20200527T065448_20200527T083618_13576_01_010302_20200528T232900.nc']],\n",
    "#    [dt(2020, 5, 28),\n",
    "#     ['data\\\\data\\\\S5P_OFFL_L2__CH4____20200528T045418_20200528T063548_13589_01_010302_20200529T212956.nc',\n",
    "#      'data\\\\data\\\\S5P_OFFL_L2__CH4____20200528T063548_20200528T081718_13590_01_010302_20200529T232047.nc']],\n",
    "#    [dt(2020, 5, 29),\n",
    "#     ['data\\\\data\\\\S5P_OFFL_L2__CH4____20200529T043518_20200529T061648_13603_01_010302_20200530T210438.nc',\n",
    "#      'data\\\\data\\\\S5P_OFFL_L2__CH4____20200529T061648_20200529T075819_13604_01_010302_20200530T224718.nc']],\n",
    "#    [dt(2020, 5, 30),\n",
    "#     ['data\\\\data\\\\S5P_OFFL_L2__CH4____20200530T055749_20200530T073919_13618_01_010302_20200531T224030.nc']],\n",
    "#    [dt(2020, 5, 31),\n",
    "#     ['data\\\\data\\\\S5P_OFFL_L2__CH4____20200531T053849_20200531T072019_13632_01_010302_20200601T222308.nc',\n",
    "#      'data\\\\data\\\\S5P_OFFL_L2__CH4____20200531T072019_20200531T090149_13633_01_010302_20200602T001734.nc']],\n",
    "#    [dt(2020, 6, 1),\n",
    "#     ['data\\\\data\\\\S5P_OFFL_L2__CH4____20200601T051949_20200601T070119_13646_01_010302_20200602T221321.nc',\n",
    "#      'data\\\\data\\\\S5P_OFFL_L2__CH4____20200601T070119_20200601T084249_13647_01_010302_20200602T233827.nc']],\n",
    "#    [dt(2020, 6, 2),\n",
    "#     ['data\\\\data\\\\S5P_OFFL_L2__CH4____20200602T050048_20200602T064218_13660_01_010302_20200603T215119.nc',\n",
    "#      'data\\\\data\\\\S5P_OFFL_L2__CH4____20200602T064218_20200602T082348_13661_01_010302_20200603T233138.nc']],\n",
    "#    [dt(2020, 6, 3),\n",
    "#     ['data\\\\data\\\\S5P_OFFL_L2__CH4____20200603T044148_20200603T062318_13674_01_010302_20200604T212730.nc',\n",
    "#      'data\\\\data\\\\S5P_OFFL_L2__CH4____20200603T062318_20200603T080448_13675_01_010302_20200604T230119.nc']],\n",
    "#    [dt(2020, 6, 4),\n",
    "#     ['data\\\\data\\\\S5P_OFFL_L2__CH4____20200604T060418_20200604T074548_13689_01_010302_20200605T225101.nc']],\n",
    "#    [dt(2020, 6, 5),\n",
    "#     ['data\\\\data\\\\S5P_OFFL_L2__CH4____20200605T054517_20200605T072647_13703_01_010302_20200606T222212.nc',\n",
    "#      'data\\\\data\\\\S5P_OFFL_L2__CH4____20200605T072647_20200605T090817_13704_01_010302_20200607T001647.nc']],\n",
    "#    [dt(2020, 6, 6),\n",
    "#     ['data\\\\data\\\\S5P_OFFL_L2__CH4____20200606T052616_20200606T070746_13717_01_010302_20200607T215556.nc',\n",
    "#      'data\\\\data\\\\S5P_OFFL_L2__CH4____20200606T070746_20200606T084916_13718_01_010302_20200607T234433.nc']]]]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
